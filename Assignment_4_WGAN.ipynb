{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.utils as vutils\nimport torchvision.transforms as transforms\nimport tensorboardX\nfrom torch.utils.data import DataLoader\nimport medmnist\nfrom torch_fidelity import calculate_metrics\nimport os\nimport shutil\nimport datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:06.513585Z","iopub.execute_input":"2025-03-29T09:23:06.513915Z","iopub.status.idle":"2025-03-29T09:23:29.362135Z","shell.execute_reply.started":"2025-03-29T09:23:06.513882Z","shell.execute_reply":"2025-03-29T09:23:29.361175Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"writer = tensorboardX.SummaryWriter(\"runs/WGAN_Pneumonia\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:29.363160Z","iopub.execute_input":"2025-03-29T09:23:29.363620Z","iopub.status.idle":"2025-03-29T09:23:29.368726Z","shell.execute_reply.started":"2025-03-29T09:23:29.363597Z","shell.execute_reply":"2025-03-29T09:23:29.368021Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class WGANLoss(nn.Module):\n    def __init__(self):\n        super(WGANLoss, self).__init__()\n    \n    def forward(self, pred, target):\n        return -torch.mean(pred) if target else torch.mean(pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:29.369977Z","iopub.execute_input":"2025-03-29T09:23:29.370338Z","iopub.status.idle":"2025-03-29T09:23:29.393664Z","shell.execute_reply.started":"2025-03-29T09:23:29.370303Z","shell.execute_reply":"2025-03-29T09:23:29.392595Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"dataset_name = \"pneumoniamnist\"\ninfo = medmnist.INFO[dataset_name]\nDataClass = getattr(medmnist, info[\"python_class\"])\n\nimage_size = 28  # PneumoniaMNIST images are 28x28\nnChannels = 1  # Grayscale images\n\ndata_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\ndataset = DataClass(split=\"train\", transform=data_transform, download=True)\nbatch_size = min(128, len(dataset)) if len(dataset) >= 128 else len(dataset)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:29.395948Z","iopub.execute_input":"2025-03-29T09:23:29.396227Z","iopub.status.idle":"2025-03-29T09:23:31.346736Z","shell.execute_reply.started":"2025-03-29T09:23:29.396199Z","shell.execute_reply":"2025-03-29T09:23:31.345849Z"}},"outputs":[{"name":"stdout","text":"Downloading https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1 to /root/.medmnist/pneumoniamnist.npz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4.17M/4.17M [00:00<00:00, 4.91MB/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"nz = 100\nngf = 64\nndf = 64\nlr = 0.00005  \nnum_epochs = 50\nn_critic = 5\nclip_value = 0.01  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:31.347788Z","iopub.execute_input":"2025-03-29T09:23:31.348029Z","iopub.status.idle":"2025-03-29T09:23:31.352119Z","shell.execute_reply.started":"2025-03-29T09:23:31.348008Z","shell.execute_reply":"2025-03-29T09:23:31.351215Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, nz, ngf, nChannels):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nChannels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n    \n    def forward(self, input):\n        return self.main(input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:31.353134Z","iopub.execute_input":"2025-03-29T09:23:31.353473Z","iopub.status.idle":"2025-03-29T09:23:31.374870Z","shell.execute_reply.started":"2025-03-29T09:23:31.353421Z","shell.execute_reply":"2025-03-29T09:23:31.374189Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Critic(nn.Module):\n    def __init__(self, ndf=64, nChannels=1):  # ndf: base channels, nChannels: input channels (grayscale=1)\n        super(Critic, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(nChannels, ndf, 3, 2, 1, bias=False),  # Output: (28x28) → (14x14)\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf, ndf * 2, 3, 2, 1, bias=False),  # Output: (14x14) → (7x7)\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),  # Output: (7x7) → (4x4)\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False)  # Output: (4x4) → (1x1)\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:31.375708Z","iopub.execute_input":"2025-03-29T09:23:31.375994Z","iopub.status.idle":"2025-03-29T09:23:31.399140Z","shell.execute_reply.started":"2025-03-29T09:23:31.375966Z","shell.execute_reply":"2025-03-29T09:23:31.398509Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"generator = Generator(nz, ngf, nChannels).cuda()\ncritic = Critic(ndf, nChannels).cuda()\n\noptimizerG = optim.RMSprop(generator.parameters(), lr=lr)\noptimizerD = optim.RMSprop(critic.parameters(), lr=lr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:31.399992Z","iopub.execute_input":"2025-03-29T09:23:31.400266Z","iopub.status.idle":"2025-03-29T09:23:31.877078Z","shell.execute_reply.started":"2025-03-29T09:23:31.400237Z","shell.execute_reply":"2025-03-29T09:23:31.876396Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"os.makedirs(\"generated_images\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:31.877828Z","iopub.execute_input":"2025-03-29T09:23:31.878045Z","iopub.status.idle":"2025-03-29T09:23:31.881941Z","shell.execute_reply.started":"2025-03-29T09:23:31.878027Z","shell.execute_reply":"2025-03-29T09:23:31.881039Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    for i, (data, _) in enumerate(dataloader):\n        real_images = data.cuda()\n        batch_size = real_images.size(0)\n\n        # Train Critic\n        for _ in range(n_critic):\n            optimizerD.zero_grad()\n            noise = torch.randn(batch_size, nz, 1, 1).cuda()\n            fake_images = generator(noise).detach()\n\n            loss_D_real = WGANLoss()(critic(real_images), True)\n            loss_D_fake = WGANLoss()(critic(fake_images), False)\n            loss_D = loss_D_real + loss_D_fake\n            loss_D.backward()\n            optimizerD.step()\n\n            # Weight Clipping\n            for p in critic.parameters():\n                p.data.clamp_(-clip_value, clip_value)\n\n        # Train Generator\n        optimizerG.zero_grad()\n        fake_images = generator(noise)\n        loss_G = WGANLoss()(critic(fake_images), True)\n        loss_G.backward()\n        optimizerG.step()\n\n        # Logging\n        writer.add_scalar(\"Loss/Discriminator\", loss_D.item(), epoch * len(dataloader) + i)\n        writer.add_scalar(\"Loss/Generator\", loss_G.item(), epoch * len(dataloader) + i)\n\n    # Save Checkpoints\n    os.makedirs(\"checkpoints\", exist_ok=True)\n    torch.save(generator.state_dict(), f'checkpoints/generator_epoch_{epoch}.pth')\n    torch.save(critic.state_dict(), f'checkpoints/critic_epoch_{epoch}.pth')\n\n    # Save Generated Images\n    os.makedirs(\"generated_images\", exist_ok=True)\n    vutils.save_image(fake_images[:16], f\"generated_images/epoch_{epoch}.png\", normalize=True)\n\n    # ✅ Add image to TensorBoard (Fixed for single slider view)\n    writer.add_image('Generated Images', vutils.make_grid(fake_images[:16], normalize=True, scale_each=True), global_step=epoch)\n\n    # Evaluate FID & IS\n    if (epoch + 1) % 10 == 0:\n        fake_images_dir = \"generated_images/fake\"\n        real_images_dir = \"generated_images/real\"\n        os.makedirs(fake_images_dir, exist_ok=True)\n        os.makedirs(real_images_dir, exist_ok=True)\n\n        # Clear old images\n        shutil.rmtree(fake_images_dir)\n        shutil.rmtree(real_images_dir)\n        os.makedirs(fake_images_dir)\n        os.makedirs(real_images_dir)\n\n        for j, img in enumerate(fake_images[:batch_size]):\n            vutils.save_image(img, os.path.join(fake_images_dir, f\"fake_{epoch}_{j}.png\"), normalize=True)\n\n        for j, img in enumerate(real_images[:batch_size]):\n            vutils.save_image(img, os.path.join(real_images_dir, f\"real_{epoch}_{j}.png\"), normalize=True)\n\n        # Compute Metrics\n        metrics = calculate_metrics(\n            input1=fake_images_dir, \n            input2=real_images_dir, \n            cuda=True, \n            isc=True, \n            fid=True\n        )\n\n        print(f\"\\n=== Epoch {epoch+1} ===\")\n        print(f\"Inception Score (IS)   : {metrics['inception_score_mean']:.4f} ± {metrics.get('inception_score_std', 0):.4f}\")\n        print(f\"Fréchet Inception Distance (FID): {metrics['frechet_inception_distance']:.4f}\")\n        print(\"=========================\")\n\n        writer.add_scalar(\"Metrics/IS\", metrics['inception_score_mean'], epoch)\n        writer.add_scalar(\"Metrics/FID\", metrics['frechet_inception_distance'], epoch)\n\nwriter.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:23:31.882839Z","iopub.execute_input":"2025-03-29T09:23:31.883042Z","iopub.status.idle":"2025-03-29T09:27:02.607138Z","shell.execute_reply.started":"2025-03-29T09:23:31.883024Z","shell.execute_reply":"2025-03-29T09:27:02.606070Z"}},"outputs":[{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nDownloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n100%|██████████| 91.2M/91.2M [00:00<00:00, 254MB/s]\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\n/usr/local/lib/python3.10/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.3727737345510767 ± 0.07945076057430353\nFrechet Inception Distance: 368.793522043068\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 10 ===\nInception Score (IS)   : 1.3728 ± 0.0795\nFréchet Inception Distance (FID): 368.7935\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.2579949406021969 ± 0.07615106716101969\nFrechet Inception Distance: 493.12889186798293\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 20 ===\nInception Score (IS)   : 1.2580 ± 0.0762\nFréchet Inception Distance (FID): 493.1289\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.2713628503282954 ± 0.1034480635000023\nFrechet Inception Distance: 404.52882882312156\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 30 ===\nInception Score (IS)   : 1.2714 ± 0.1034\nFréchet Inception Distance (FID): 404.5288\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.2843373805005955 ± 0.06183867830910506\nFrechet Inception Distance: 393.09944614076903\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 40 ===\nInception Score (IS)   : 1.2843 ± 0.0618\nFréchet Inception Distance (FID): 393.0994\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.2647722571411362 ± 0.060447090625046045\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 50 ===\nInception Score (IS)   : 1.2648 ± 0.0604\nFréchet Inception Distance (FID): 368.7797\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Frechet Inception Distance: 368.77974013272154\n","output_type":"stream"}],"execution_count":14}]}