{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.utils as vutils\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader\nimport medmnist\nfrom torch_fidelity import calculate_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:55:45.145582Z","iopub.execute_input":"2025-03-29T05:55:45.146023Z","iopub.status.idle":"2025-03-29T05:56:02.383328Z","shell.execute_reply.started":"2025-03-29T05:55:45.145992Z","shell.execute_reply":"2025-03-29T05:56:02.382332Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"writer = SummaryWriter(\"runs/LS_GAN_PneumoniaMNIST\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:56:02.384456Z","iopub.execute_input":"2025-03-29T05:56:02.385067Z","iopub.status.idle":"2025-03-29T05:56:02.389877Z","shell.execute_reply.started":"2025-03-29T05:56:02.385041Z","shell.execute_reply":"2025-03-29T05:56:02.389052Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class LSLoss(nn.Module):\n    def __init__(self):\n        super(LSLoss, self).__init__()\n    \n    def forward(self, pred, target):\n        return torch.mean((pred - target) ** 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:56:02.391295Z","iopub.execute_input":"2025-03-29T05:56:02.391577Z","iopub.status.idle":"2025-03-29T05:56:02.425317Z","shell.execute_reply.started":"2025-03-29T05:56:02.391556Z","shell.execute_reply":"2025-03-29T05:56:02.424707Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset_name = \"pneumoniamnist\"\ninfo = medmnist.INFO[dataset_name]\nDataClass = getattr(medmnist, info[\"python_class\"])\n\nimage_size = 28  # PneumoniaMNIST images are 28x28\nnChannels = 1  # Grayscale images\n\ndata_transform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\ndataset = DataClass(split=\"train\", transform=data_transform, download=True)\nbatch_size = min(128, len(dataset)) if len(dataset) >= 128 else len(dataset)  # Adjust batch size dynamically\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:56:02.426295Z","iopub.execute_input":"2025-03-29T05:56:02.426573Z","iopub.status.idle":"2025-03-29T05:56:04.314155Z","shell.execute_reply.started":"2025-03-29T05:56:02.426545Z","shell.execute_reply":"2025-03-29T05:56:04.313193Z"}},"outputs":[{"name":"stdout","text":"Downloading https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1 to /root/.medmnist/pneumoniamnist.npz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4.17M/4.17M [00:00<00:00, 4.71MB/s]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"nz = 100\nngf = 64\nndf = 64\nlr = 0.0002\nnum_epochs = 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:56:19.524568Z","iopub.execute_input":"2025-03-29T05:56:19.524900Z","iopub.status.idle":"2025-03-29T05:56:19.528476Z","shell.execute_reply.started":"2025-03-29T05:56:19.524876Z","shell.execute_reply":"2025-03-29T05:56:19.527736Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, nz, ngf, nChannels):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            \n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            \n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            \n            nn.ConvTranspose2d(ngf, nChannels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n    \n    def forward(self, input):\n        return self.main(input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:56:20.542011Z","iopub.execute_input":"2025-03-29T05:56:20.542289Z","iopub.status.idle":"2025-03-29T05:56:20.547863Z","shell.execute_reply.started":"2025-03-29T05:56:20.542266Z","shell.execute_reply":"2025-03-29T05:56:20.546923Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, ndf, nChannels):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(nChannels, ndf, 3, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(ndf, ndf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(ndf * 2, 1, 7, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, input):\n        return self.main(input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:56:22.172991Z","iopub.execute_input":"2025-03-29T05:56:22.173305Z","iopub.status.idle":"2025-03-29T05:56:22.178270Z","shell.execute_reply.started":"2025-03-29T05:56:22.173278Z","shell.execute_reply":"2025-03-29T05:56:22.177580Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"generator = Generator(nz, ngf, nChannels).cuda()\ndiscriminator = Discriminator(ndf, nChannels).cuda()\n\n# Optimizers\noptimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\ncriterion = LSLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:56:23.454056Z","iopub.execute_input":"2025-03-29T05:56:23.454390Z","iopub.status.idle":"2025-03-29T05:56:23.930982Z","shell.execute_reply.started":"2025-03-29T05:56:23.454363Z","shell.execute_reply":"2025-03-29T05:56:23.930314Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nos.makedirs(\"generated_images\", exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:56:32.428325Z","iopub.execute_input":"2025-03-29T05:56:32.428634Z","iopub.status.idle":"2025-03-29T05:56:32.432290Z","shell.execute_reply.started":"2025-03-29T05:56:32.428608Z","shell.execute_reply":"2025-03-29T05:56:32.431584Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import datetime\n\nfor epoch in range(num_epochs):\n    \n    # Train the model\n    for i, (data, _) in enumerate(dataloader):\n        real_images = data.cuda()\n        batch_size = real_images.size(0)\n        real_labels = torch.ones(batch_size, 1, 1, 1).cuda()\n        fake_labels = torch.zeros(batch_size, 1, 1, 1).cuda()\n\n        # Train Discriminator\n        optimizerD.zero_grad()\n        output_real = discriminator(real_images)\n        loss_real = criterion(output_real, real_labels)\n\n        noise = torch.randn(batch_size, nz, 1, 1).cuda()\n        fake_images = generator(noise)\n        output_fake = discriminator(fake_images.detach())\n        loss_fake = criterion(output_fake, fake_labels)\n\n        loss_D = (loss_real + loss_fake) / 2\n        loss_D.backward()\n        optimizerD.step()\n\n        # Train Generator\n        optimizerG.zero_grad()\n        output_fake = discriminator(fake_images)\n        loss_G = criterion(output_fake, real_labels)\n        loss_G.backward()\n        optimizerG.step()\n\n        writer.add_scalar(\"Loss/Discriminator\", loss_D.item(), epoch * len(dataloader) + i)\n        writer.add_scalar(\"Loss/Generator\", loss_G.item(), epoch * len(dataloader) + i)\n\n\n        # Save generated images every 100 batches\n        if i % 100 == 0:\n            writer.add_images(\"Generated Images\", fake_images[:16], global_step=epoch)\n            vutils.save_image(fake_images[:16], f\"generated_images/epoch_{epoch}_batch_{i}.png\", normalize=True)\n\n    # Save Model Checkpoints\n    torch.save(generator.state_dict(), f'generator_epoch_{epoch}.pth')\n    torch.save(discriminator.state_dict(), f'discriminator_epoch_{epoch}.pth')\n\n    # Save generated images for FID/IS evaluation\n    fake_images_dir = \"generated_images/fake\"\n    real_images_dir = \"generated_images/real\"\n    os.makedirs(fake_images_dir, exist_ok=True)\n    os.makedirs(real_images_dir, exist_ok=True)\n\n    for j, img in enumerate(fake_images[:batch_size]):\n        vutils.save_image(img, os.path.join(fake_images_dir, f\"fake_{epoch}_{j}.png\"), normalize=True)\n\n    for j, img in enumerate(real_images[:batch_size]):\n        vutils.save_image(img, os.path.join(real_images_dir, f\"real_{epoch}_{j}.png\"), normalize=True)\n\n    # Print only after every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        \n\n        timestamp = datetime.datetime.now().strftime(\"%H:%M:%S\")\n        print(f\"\\n{'='*50}\")\n        print(f\" Epoch {epoch+1}/{num_epochs} | Training Progress \")\n        print(f\"{'='*50}\")\n        print(f\"[{timestamp}] Loss D: {loss_D.item():.4f} | Loss G: {loss_G.item():.4f}\")\n\n        # Evaluate IS and FID\n        metrics = calculate_metrics(\n            input1=fake_images_dir, \n            input2=real_images_dir, \n            cuda=True, isc=True, fid=True\n        )\n\n        print(f\"\\n{'-'*50}\")\n        print(f\" Evaluation Metrics (Epoch {epoch+1})\")\n        print(f\"{'-'*50}\")\n        print(f\" Inception Score (IS) : {metrics['inception_score_mean']:.4f} ± {metrics['inception_score_std']:.4f}\")\n        print(f\" Fréchet Inception Distance (FID): {metrics['frechet_inception_distance']:.4f}\")\n        print(f\"{'='*50}\\n\")\n\n        writer.add_scalar(\"Metrics/Inception Score\", metrics['inception_score_mean'], epoch)\n        writer.add_scalar(\"Metrics/FID\", metrics['frechet_inception_distance'], epoch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T05:57:34.838225Z","iopub.execute_input":"2025-03-29T05:57:34.838622Z","iopub.status.idle":"2025-03-29T06:01:31.569567Z","shell.execute_reply.started":"2025-03-29T05:57:34.838582Z","shell.execute_reply":"2025-03-29T06:01:31.568461Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\n Epoch 10/50 | Training Progress \n==================================================\n[05:57:50] Loss D: 0.0003 | Loss G: 0.9758\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nDownloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n100%|██████████| 91.2M/91.2M [00:00<00:00, 259MB/s]\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 1000 samples\n/usr/local/lib/python3.10/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\nProcessing samples                                                           \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 1000 samples\nProcessing samples                                                           \nInception Score: 2.122189435235305 ± 0.08656892939482286\nFrechet Inception Distance: 287.01383491032703\n","output_type":"stream"},{"name":"stdout","text":"\n--------------------------------------------------\n Evaluation Metrics (Epoch 10)\n--------------------------------------------------\n Inception Score (IS) : 2.1222 ± 0.0866\n Fréchet Inception Distance (FID): 287.0138\n==================================================\n\n\n==================================================\n Epoch 20/50 | Training Progress \n==================================================\n[05:58:23] Loss D: 0.1012 | Loss G: 0.8669\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 2000 samples\nProcessing samples                                                           \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 2000 samples\nProcessing samples                                                           \nInception Score: 2.4046504730089615 ± 0.06989617184050663\nFrechet Inception Distance: 299.2838076640951\n","output_type":"stream"},{"name":"stdout","text":"\n--------------------------------------------------\n Evaluation Metrics (Epoch 20)\n--------------------------------------------------\n Inception Score (IS) : 2.4047 ± 0.0699\n Fréchet Inception Distance (FID): 299.2838\n==================================================\n\n\n==================================================\n Epoch 30/50 | Training Progress \n==================================================\n[05:59:04] Loss D: 0.0848 | Loss G: 0.8000\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 3000 samples\nProcessing samples                                                           \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 3000 samples\nProcessing samples                                                           \nInception Score: 3.082019988497799 ± 0.07072949314313365\nFrechet Inception Distance: 215.4396624128606\n","output_type":"stream"},{"name":"stdout","text":"\n--------------------------------------------------\n Evaluation Metrics (Epoch 30)\n--------------------------------------------------\n Inception Score (IS) : 3.0820 ± 0.0707\n Fréchet Inception Distance (FID): 215.4397\n==================================================\n\n\n==================================================\n Epoch 40/50 | Training Progress \n==================================================\n[05:59:49] Loss D: 0.0734 | Loss G: 0.7932\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 4000 samples\nProcessing samples                                                           \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 4000 samples\nProcessing samples                                                           \nInception Score: 3.1534999774749624 ± 0.052210753024167104\nFrechet Inception Distance: 180.27684714523156\n","output_type":"stream"},{"name":"stdout","text":"\n--------------------------------------------------\n Evaluation Metrics (Epoch 40)\n--------------------------------------------------\n Inception Score (IS) : 3.1535 ± 0.0522\n Fréchet Inception Distance (FID): 180.2768\n==================================================\n\n\n==================================================\n Epoch 50/50 | Training Progress \n==================================================\n[06:00:44] Loss D: 0.0719 | Loss G: 0.7946\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 5000 samples\nProcessing samples                                                           \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 5000 samples\nProcessing samples                                                           \nInception Score: 3.0573660537646736 ± 0.07029686617252294\n","output_type":"stream"},{"name":"stdout","text":"\n--------------------------------------------------\n Evaluation Metrics (Epoch 50)\n--------------------------------------------------\n Inception Score (IS) : 3.0574 ± 0.0703\n Fréchet Inception Distance (FID): 160.0485\n==================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Frechet Inception Distance: 160.04852101666927\n","output_type":"stream"}],"execution_count":22}]}