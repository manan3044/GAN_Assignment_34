{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.utils as vutils\nimport torchvision.transforms as transforms\nimport tensorboardX\nfrom torch.utils.data import DataLoader\nimport medmnist\nfrom torch_fidelity import calculate_metrics\nimport os\nimport shutil\nimport torch.autograd as autograd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:12:58.201383Z","iopub.execute_input":"2025-03-29T09:12:58.201653Z","iopub.status.idle":"2025-03-29T09:13:17.092926Z","shell.execute_reply.started":"2025-03-29T09:12:58.201630Z","shell.execute_reply":"2025-03-29T09:13:17.092232Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def compute_gradient_penalty(critic, real_images, fake_images):\n    alpha = torch.rand(real_images.size(0), 1, 1, 1).cuda()\n    interpolates = (alpha * real_images + (1 - alpha) * fake_images).requires_grad_(True)\n    critic_interpolates = critic(interpolates)\n    gradients = autograd.grad(outputs=critic_interpolates, inputs=interpolates,\n                              grad_outputs=torch.ones(critic_interpolates.size()).cuda(),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n\nclass WGAN_GP_Loss(nn.Module):\n    def __init__(self):\n        super(WGAN_GP_Loss, self).__init__()\n    def forward(self, pred, target):\n        return -torch.mean(pred) if target else torch.mean(pred)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:13:17.093775Z","iopub.execute_input":"2025-03-29T09:13:17.094356Z","iopub.status.idle":"2025-03-29T09:13:17.100422Z","shell.execute_reply.started":"2025-03-29T09:13:17.094326Z","shell.execute_reply":"2025-03-29T09:13:17.099586Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset_name = \"pneumoniamnist\"\ninfo = medmnist.INFO[dataset_name]\nDataClass = getattr(medmnist, info[\"python_class\"])\n\nimage_size = 28  \nnChannels = 1  \n\ntransform = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\ndataset = DataClass(split=\"train\", transform=transform, download=True)\nbatch_size = 128\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:13:17.101430Z","iopub.execute_input":"2025-03-29T09:13:17.101764Z","iopub.status.idle":"2025-03-29T09:13:18.926065Z","shell.execute_reply.started":"2025-03-29T09:13:17.101733Z","shell.execute_reply":"2025-03-29T09:13:18.925239Z"}},"outputs":[{"name":"stdout","text":"Downloading https://zenodo.org/records/10519652/files/pneumoniamnist.npz?download=1 to /root/.medmnist/pneumoniamnist.npz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4.17M/4.17M [00:00<00:00, 4.74MB/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"nz = 100\nngf = 64\nndf = 64\nlr = 0.0001  \nnum_epochs = 50\nn_critic = 5\ngp_lambda = 10  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:13:18.928489Z","iopub.execute_input":"2025-03-29T09:13:18.928707Z","iopub.status.idle":"2025-03-29T09:13:18.932590Z","shell.execute_reply.started":"2025-03-29T09:13:18.928689Z","shell.execute_reply":"2025-03-29T09:13:18.931870Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, nz, ngf, nChannels):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),  # Output: 4x4\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),  # Output: 8x8\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf * 2, ngf, 3, 2, 1, bias=False),  # Output: 16x16\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf, nChannels, 4, 2, 1, bias=False),  # Output: 32x32\n            nn.Tanh(),\n\n            nn.Upsample(size=(28, 28), mode='bilinear', align_corners=True)  # Resize to 28x28\n        )\n\n    def forward(self, input):\n        return self.main(input)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:13:18.933654Z","iopub.execute_input":"2025-03-29T09:13:18.933898Z","iopub.status.idle":"2025-03-29T09:13:18.953513Z","shell.execute_reply.started":"2025-03-29T09:13:18.933878Z","shell.execute_reply":"2025-03-29T09:13:18.952766Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Critic(nn.Module):\n    def __init__(self, ndf=64, nChannels=1):\n        super(Critic, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(nChannels, ndf, 3, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False)\n        )\n    def forward(self, input):\n        return self.main(input).view(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:13:18.954106Z","iopub.execute_input":"2025-03-29T09:13:18.954316Z","iopub.status.idle":"2025-03-29T09:13:18.976450Z","shell.execute_reply.started":"2025-03-29T09:13:18.954298Z","shell.execute_reply":"2025-03-29T09:13:18.975824Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"generator = Generator(nz, ngf, nChannels).cuda()\ncritic = Critic(ndf, nChannels).cuda()\noptimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(0, 0.9))\noptimizerD = optim.Adam(critic.parameters(), lr=lr, betas=(0, 0.9))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:13:18.977323Z","iopub.execute_input":"2025-03-29T09:13:18.977577Z","iopub.status.idle":"2025-03-29T09:13:19.403471Z","shell.execute_reply.started":"2025-03-29T09:13:18.977549Z","shell.execute_reply":"2025-03-29T09:13:19.402692Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"writer = tensorboardX.SummaryWriter(\"runs/WGAN_GP_Pneumonia\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:13:19.404283Z","iopub.execute_input":"2025-03-29T09:13:19.404501Z","iopub.status.idle":"2025-03-29T09:13:19.408954Z","shell.execute_reply.started":"2025-03-29T09:13:19.404483Z","shell.execute_reply":"2025-03-29T09:13:19.408390Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"os.makedirs(\"checkpoints\", exist_ok=True)\nos.makedirs(\"generated_images\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:13:19.409675Z","iopub.execute_input":"2025-03-29T09:13:19.409892Z","iopub.status.idle":"2025-03-29T09:13:19.426478Z","shell.execute_reply.started":"2025-03-29T09:13:19.409873Z","shell.execute_reply":"2025-03-29T09:13:19.425712Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    for i, (data, _) in enumerate(dataloader):\n        real_images = data.cuda()\n        batch_size = real_images.size(0)\n\n        # Train Critic\n        for _ in range(n_critic):\n            optimizerD.zero_grad()\n            noise = torch.randn(batch_size, nz, 1, 1).cuda()\n            fake_images = generator(noise).detach()\n\n            loss_D_real = WGAN_GP_Loss()(critic(real_images), True)\n            loss_D_fake = WGAN_GP_Loss()(critic(fake_images), False)\n            gradient_penalty = compute_gradient_penalty(critic, real_images, fake_images)\n            loss_D = loss_D_real + loss_D_fake + gp_lambda * gradient_penalty\n            loss_D.backward()\n            optimizerD.step()\n\n        # Train Generator\n        optimizerG.zero_grad()\n        fake_images = generator(noise)\n        loss_G = WGAN_GP_Loss()(critic(fake_images), True)\n        loss_G.backward()\n        optimizerG.step()\n\n        # Logging\n        writer.add_scalar(\"Loss/Discriminator\", loss_D.item(), epoch * len(dataloader) + i)\n        writer.add_scalar(\"Loss/Generator\", loss_G.item(), epoch * len(dataloader) + i)\n\n    # Save Checkpoints\n    os.makedirs(\"checkpoints\", exist_ok=True)\n    torch.save(generator.state_dict(), f'checkpoints/generator_epoch_{epoch}.pth')\n    torch.save(critic.state_dict(), f'checkpoints/critic_epoch_{epoch}.pth')\n\n    # Save Generated Images\n    os.makedirs(\"generated_images\", exist_ok=True)\n    vutils.save_image(fake_images[:16], f\"generated_images/epoch_{epoch}.png\", normalize=True)\n    writer.add_image('Generated Images', vutils.make_grid(fake_images[:16], normalize=True, scale_each=True), global_step=epoch)\n\n\n    # Evaluate FID & IS every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        fake_images_dir = \"generated_images/fake\"\n        real_images_dir = \"generated_images/real\"\n        os.makedirs(fake_images_dir, exist_ok=True)\n        os.makedirs(real_images_dir, exist_ok=True)\n\n        # Clear old images\n        shutil.rmtree(fake_images_dir)\n        shutil.rmtree(real_images_dir)\n        os.makedirs(fake_images_dir)\n        os.makedirs(real_images_dir)\n\n        for j, img in enumerate(fake_images[:batch_size]):\n            vutils.save_image(img, os.path.join(fake_images_dir, f\"fake_{epoch}_{j}.png\"), normalize=True)\n\n        for j, img in enumerate(real_images[:batch_size]):\n            vutils.save_image(img, os.path.join(real_images_dir, f\"real_{epoch}_{j}.png\"), normalize=True)\n\n        # Compute Metrics\n        metrics = calculate_metrics(\n            input1=fake_images_dir, \n            input2=real_images_dir, \n            cuda=True, \n            isc=True, \n            fid=True\n        )\n\n        print(f\"\\n=== Epoch {epoch+1} ===\")\n        print(f\"Inception Score (IS)   : {metrics['inception_score_mean']:.4f} ± {metrics.get('inception_score_std', 0):.4f}\")\n        print(f\"Fréchet Inception Distance (FID): {metrics['frechet_inception_distance']:.4f}\")\n        print(\"=========================\")\n\n        writer.add_scalar(\"Metrics/IS\", metrics['inception_score_mean'], epoch)\n        writer.add_scalar(\"Metrics/FID\", metrics['frechet_inception_distance'], epoch)\n\nwriter.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T09:13:19.427143Z","iopub.execute_input":"2025-03-29T09:13:19.427332Z","iopub.status.idle":"2025-03-29T09:18:40.809692Z","shell.execute_reply.started":"2025-03-29T09:13:19.427315Z","shell.execute_reply":"2025-03-29T09:18:40.808588Z"}},"outputs":[{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nDownloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n100%|██████████| 91.2M/91.2M [00:00<00:00, 250MB/s]\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\n/usr/local/lib/python3.10/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.5273970943593898 ± 0.12476322417810928\nFrechet Inception Distance: 315.8864258190897\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 10 ===\nInception Score (IS)   : 1.5274 ± 0.1248\nFréchet Inception Distance (FID): 315.8864\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.6529498620246321 ± 0.13759774574990646\nFrechet Inception Distance: 312.0202794101396\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 20 ===\nInception Score (IS)   : 1.6529 ± 0.1376\nFréchet Inception Distance (FID): 312.0203\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.7129441604877846 ± 0.150796951161115\nFrechet Inception Distance: 319.5476884041691\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 30 ===\nInception Score (IS)   : 1.7129 ± 0.1508\nFréchet Inception Distance (FID): 319.5477\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.7187430765533385 ± 0.0859940585032735\nFrechet Inception Distance: 310.2978322549363\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 40 ===\nInception Score (IS)   : 1.7187 ± 0.0860\nFréchet Inception Distance (FID): 310.2978\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Creating feature extractor \"inception-v3-compat\" with features ['2048', 'logits_unbiased']\nExtracting features from input1\nLooking for samples non-recursivelty in \"generated_images/fake\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nExtracting features from input2\nLooking for samples non-recursivelty in \"generated_images/real\" with extensions png,jpg,jpeg\nFound 100 samples\nProcessing samples                                                         \nInception Score: 1.8842514485646145 ± 0.1369871670951578\n","output_type":"stream"},{"name":"stdout","text":"\n=== Epoch 50 ===\nInception Score (IS)   : 1.8843 ± 0.1370\nFréchet Inception Distance (FID): 359.8660\n=========================\n","output_type":"stream"},{"name":"stderr","text":"Frechet Inception Distance: 359.8659668463393\n","output_type":"stream"}],"execution_count":14}]}